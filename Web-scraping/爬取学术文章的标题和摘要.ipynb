{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests用于获取网页内容\n",
    "# beautifulsoup用于parse_html \n",
    "# 参考教程大全：https://blog.csdn.net/slhlde/article/details/81937838\n",
    "# mechanize用于模拟浏览器输入信息来搜索或登录:\n",
    "# 参考：https://pypi.org/project/mechanize/\n",
    "# http://stockrt.github.io/p/emulating-a-browser-in-python-with-mechanize/\n",
    "# 用time延时以免被反爬虫甄别\n",
    "# selenium教程：https://www.jianshu.com/p/6c82c965c014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反爬虫机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反反爬虫利器！教你怎么用代理，拨号换IP：https://zhuanlan.zhihu.com/p/26876834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代理池 https://www.jianshu.com/p/b76df35aec93\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://111.198.219.151:8118'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ip_pool = [\n",
    "    '119.98.44.192:8118',\n",
    "    '111.198.219.151:8118',\n",
    "    '101.86.86.101:8118',\n",
    "]\n",
    "def ip_proxy():\n",
    "    ip = ip_pool[random.randrange(0,3)]\n",
    "    proxy_ip = 'http://'+ip\n",
    "    proxies = {'http':proxy_ip}\n",
    "    return proxies\n",
    "\n",
    "print(ip_proxy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping the 5 page...\n",
      "Scraping the 6 page...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "# 参考：https://blog.csdn.net/Shenpibaipao/article/details/80217737\n",
    "# https://www.jianshu.com/p/b76df35aec93\n",
    "# https://blog.csdn.net/ysblogs/article/details/88530124\n",
    "# https://blog.csdn.net/qq_41007998/article/details/109221626\n",
    "\n",
    "for i in range(1,7):\n",
    "    headers = {'User-Agent': ua.random} # 更换headers来应对反爬虫\n",
    "    page_link = 'https://academic.oup.com/applij/search-results?q=second+language+acquisition&f_ArticleTypeDisplayName=Research+Article&fl_SiteID=5135&rg_ArticleDate=12%2f01%2f2015+TO+12%2f31%2f2020&qb=%7b%22All1%22%3a%22second+language+acquisition%22%7d&page='+str(i)\n",
    "    bs = BeautifulSoup(requests.get(page_link, headers=headers).content, \"lxml\")\n",
    "    print('Scraping the ' +str(i)+ ' page...')\n",
    "    for div in bs.find_all(\"div\", class_ = \"al-citation-list\"):  \n",
    "        headers = {'User-Agent': ua.random}\n",
    "        link = div.a.contents[0]\n",
    "        bs = BeautifulSoup(requests.get(link, headers=headers).content, \"lxml\")\n",
    "        title = bs.title.contents[0]\n",
    "        date = bs.find('div', class_ = 'citation-date').contents[0]\n",
    "        abstract = bs.find('p', class_='chapter-para').contents\n",
    "        paper_dict = {}\n",
    "        paper_dict[ 'title'] = title\n",
    "        paper_dict[ 'date'] = date\n",
    "        paper_dict[ 'abstract'] = abstract\n",
    "        paper_dict[ 'link'] = link\n",
    "        paper_list.append(paper_dict)\n",
    "        time.sleep(random.random()*10)\n",
    "    time.sleep(random.random()*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(paper_list)\n",
    "df.drop_duplicates(subset=['title','date'],keep='first',inplace=True)\n",
    "df['year'] = df['date'].str.extract('(\\d{4})$', expand=True)\n",
    "df.sort_values(by='date', inplace=True)\n",
    "df['abstract'] = [''.join(str(abstract[i]) for i in range(len(abstract))) for abstract in df['abstract']]\n",
    "df['title'] = df['title'].str.extract('^(.*) \\| Applied Linguistics', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('papers.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
